{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f1f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from numpy import savetxt\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import balanced_accuracy_score as bal_accuracy\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import asgl "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175dd469",
   "metadata": {},
   "source": [
    "## Importing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b432735",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/hughtillmanjamesjr./Desktop/Research/DL_Elbow_Logging/Fall 2022/Clustering/Euclidean/Nov6_norm_stand/kmeans_L2_info_array_30_.csv', header = None)\n",
    "Y = data.loc[:,1]\n",
    "X = data.loc[:,2:]\n",
    "X_cap = data.loc[:,2]\n",
    "X_clust = data.loc[:,11:]\n",
    "\n",
    "#normalizing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_norm = min_max_scaler.fit_transform(X_clust)\n",
    "#X_norm = min_max_scaler.fit_transform(X)\n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, Y, test_size=0.33, random_state=0)\n",
    "\n",
    "# savetxt('/Users/hughtillmanjamesjr./Desktop/Wash U Engineering/2022-2023/Fall 2022/Topics in Stat - ML/Project/X_train.csv', X_train, delimiter=',')\n",
    "# savetxt('/Users/hughtillmanjamesjr./Desktop/Wash U Engineering/2022-2023/Fall 2022/Topics in Stat - ML/Project/X_test.csv', X_test, delimiter=',')\n",
    "# savetxt('/Users/hughtillmanjamesjr./Desktop/Wash U Engineering/2022-2023/Fall 2022/Topics in Stat - ML/Project/y_train.csv', y_train, delimiter=',')\n",
    "# savetxt('/Users/hughtillmanjamesjr./Desktop/Wash U Engineering/2022-2023/Fall 2022/Topics in Stat - ML/Project/y_test.csv', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd463fb",
   "metadata": {},
   "source": [
    "## Pearson Correlation Coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f4f8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.192000</td>\n",
       "      <td>0.028072</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.693835</td>\n",
       "      <td>0.093519</td>\n",
       "      <td>0.265218</td>\n",
       "      <td>0.230291</td>\n",
       "      <td>0.224367</td>\n",
       "      <td>-0.255230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238289</td>\n",
       "      <td>-0.141079</td>\n",
       "      <td>-0.014041</td>\n",
       "      <td>-0.122167</td>\n",
       "      <td>-0.182019</td>\n",
       "      <td>-0.167571</td>\n",
       "      <td>-0.043977</td>\n",
       "      <td>-0.321889</td>\n",
       "      <td>0.260855</td>\n",
       "      <td>-0.172229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.192000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004576</td>\n",
       "      <td>-0.002516</td>\n",
       "      <td>-0.276600</td>\n",
       "      <td>-0.052171</td>\n",
       "      <td>-0.195711</td>\n",
       "      <td>-0.206691</td>\n",
       "      <td>-0.010387</td>\n",
       "      <td>0.152875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408296</td>\n",
       "      <td>0.339493</td>\n",
       "      <td>0.560328</td>\n",
       "      <td>0.631811</td>\n",
       "      <td>0.516253</td>\n",
       "      <td>0.315326</td>\n",
       "      <td>0.418234</td>\n",
       "      <td>0.631251</td>\n",
       "      <td>-0.070284</td>\n",
       "      <td>0.494303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028072</td>\n",
       "      <td>-0.004576</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.115671</td>\n",
       "      <td>0.981838</td>\n",
       "      <td>0.148160</td>\n",
       "      <td>0.132954</td>\n",
       "      <td>0.011847</td>\n",
       "      <td>0.436662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057824</td>\n",
       "      <td>-0.055608</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.139223</td>\n",
       "      <td>0.082991</td>\n",
       "      <td>-0.169752</td>\n",
       "      <td>0.033049</td>\n",
       "      <td>-0.017337</td>\n",
       "      <td>0.231881</td>\n",
       "      <td>0.115093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033332</td>\n",
       "      <td>-0.002516</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.115602</td>\n",
       "      <td>0.978863</td>\n",
       "      <td>0.144013</td>\n",
       "      <td>0.127622</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>0.430959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062134</td>\n",
       "      <td>-0.054072</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.141646</td>\n",
       "      <td>0.084914</td>\n",
       "      <td>-0.166304</td>\n",
       "      <td>0.037505</td>\n",
       "      <td>-0.017420</td>\n",
       "      <td>0.230313</td>\n",
       "      <td>0.118114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.693835</td>\n",
       "      <td>-0.276600</td>\n",
       "      <td>0.115671</td>\n",
       "      <td>0.115602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235160</td>\n",
       "      <td>0.238894</td>\n",
       "      <td>0.257863</td>\n",
       "      <td>0.237655</td>\n",
       "      <td>-0.018931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153915</td>\n",
       "      <td>0.056141</td>\n",
       "      <td>-0.094497</td>\n",
       "      <td>-0.072074</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.220223</td>\n",
       "      <td>-0.115186</td>\n",
       "      <td>-0.307936</td>\n",
       "      <td>0.187330</td>\n",
       "      <td>-0.282353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.093519</td>\n",
       "      <td>-0.052171</td>\n",
       "      <td>0.981838</td>\n",
       "      <td>0.978863</td>\n",
       "      <td>0.235160</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.196263</td>\n",
       "      <td>0.188561</td>\n",
       "      <td>0.056667</td>\n",
       "      <td>0.437328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>-0.051911</td>\n",
       "      <td>-0.003344</td>\n",
       "      <td>0.126638</td>\n",
       "      <td>0.043272</td>\n",
       "      <td>-0.201778</td>\n",
       "      <td>-0.024720</td>\n",
       "      <td>-0.024290</td>\n",
       "      <td>0.264607</td>\n",
       "      <td>0.048090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.265218</td>\n",
       "      <td>-0.195711</td>\n",
       "      <td>0.148160</td>\n",
       "      <td>0.144013</td>\n",
       "      <td>0.238894</td>\n",
       "      <td>0.196263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989281</td>\n",
       "      <td>0.722834</td>\n",
       "      <td>0.160087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093445</td>\n",
       "      <td>0.099466</td>\n",
       "      <td>-0.231918</td>\n",
       "      <td>0.029693</td>\n",
       "      <td>0.217328</td>\n",
       "      <td>-0.447725</td>\n",
       "      <td>0.135681</td>\n",
       "      <td>-0.194180</td>\n",
       "      <td>0.654221</td>\n",
       "      <td>-0.095970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.230291</td>\n",
       "      <td>-0.206691</td>\n",
       "      <td>0.132954</td>\n",
       "      <td>0.127622</td>\n",
       "      <td>0.257863</td>\n",
       "      <td>0.188561</td>\n",
       "      <td>0.989281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730466</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079379</td>\n",
       "      <td>0.126370</td>\n",
       "      <td>-0.250841</td>\n",
       "      <td>0.028196</td>\n",
       "      <td>0.237765</td>\n",
       "      <td>-0.466282</td>\n",
       "      <td>0.143559</td>\n",
       "      <td>-0.203930</td>\n",
       "      <td>0.622073</td>\n",
       "      <td>-0.126762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.224367</td>\n",
       "      <td>-0.010387</td>\n",
       "      <td>0.011847</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>0.237655</td>\n",
       "      <td>0.056667</td>\n",
       "      <td>0.722834</td>\n",
       "      <td>0.730466</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.065224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048653</td>\n",
       "      <td>0.209222</td>\n",
       "      <td>-0.106415</td>\n",
       "      <td>0.064280</td>\n",
       "      <td>0.248623</td>\n",
       "      <td>-0.332443</td>\n",
       "      <td>0.177048</td>\n",
       "      <td>-0.086194</td>\n",
       "      <td>0.473402</td>\n",
       "      <td>-0.078167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.255230</td>\n",
       "      <td>0.152875</td>\n",
       "      <td>0.436662</td>\n",
       "      <td>0.430959</td>\n",
       "      <td>-0.018931</td>\n",
       "      <td>0.437328</td>\n",
       "      <td>0.160087</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.065224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101415</td>\n",
       "      <td>0.279416</td>\n",
       "      <td>-0.149579</td>\n",
       "      <td>0.207532</td>\n",
       "      <td>0.480209</td>\n",
       "      <td>-0.040215</td>\n",
       "      <td>0.207686</td>\n",
       "      <td>-0.042101</td>\n",
       "      <td>0.030940</td>\n",
       "      <td>0.070622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.174986</td>\n",
       "      <td>0.516696</td>\n",
       "      <td>-0.112593</td>\n",
       "      <td>-0.111705</td>\n",
       "      <td>-0.287176</td>\n",
       "      <td>-0.118538</td>\n",
       "      <td>-0.458702</td>\n",
       "      <td>-0.477018</td>\n",
       "      <td>-0.305232</td>\n",
       "      <td>-0.231900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162299</td>\n",
       "      <td>-0.012380</td>\n",
       "      <td>0.464235</td>\n",
       "      <td>0.185894</td>\n",
       "      <td>-0.129755</td>\n",
       "      <td>0.449549</td>\n",
       "      <td>-0.173424</td>\n",
       "      <td>0.578557</td>\n",
       "      <td>-0.245426</td>\n",
       "      <td>0.112362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.231308</td>\n",
       "      <td>0.620085</td>\n",
       "      <td>0.140608</td>\n",
       "      <td>0.143014</td>\n",
       "      <td>-0.149877</td>\n",
       "      <td>0.099519</td>\n",
       "      <td>0.154098</td>\n",
       "      <td>0.163338</td>\n",
       "      <td>0.205343</td>\n",
       "      <td>0.388576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306133</td>\n",
       "      <td>0.465113</td>\n",
       "      <td>0.199596</td>\n",
       "      <td>0.618593</td>\n",
       "      <td>0.692945</td>\n",
       "      <td>-0.039766</td>\n",
       "      <td>0.567402</td>\n",
       "      <td>0.391891</td>\n",
       "      <td>0.116025</td>\n",
       "      <td>0.406802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.251006</td>\n",
       "      <td>0.428826</td>\n",
       "      <td>-0.075485</td>\n",
       "      <td>-0.068866</td>\n",
       "      <td>0.141796</td>\n",
       "      <td>-0.095877</td>\n",
       "      <td>-0.523346</td>\n",
       "      <td>-0.533989</td>\n",
       "      <td>-0.275460</td>\n",
       "      <td>-0.171296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348507</td>\n",
       "      <td>0.018992</td>\n",
       "      <td>0.282084</td>\n",
       "      <td>0.087681</td>\n",
       "      <td>-0.028771</td>\n",
       "      <td>0.396340</td>\n",
       "      <td>-0.023487</td>\n",
       "      <td>0.094099</td>\n",
       "      <td>-0.334810</td>\n",
       "      <td>0.026669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.023986</td>\n",
       "      <td>0.469636</td>\n",
       "      <td>-0.035573</td>\n",
       "      <td>-0.034890</td>\n",
       "      <td>-0.017807</td>\n",
       "      <td>-0.018516</td>\n",
       "      <td>-0.327769</td>\n",
       "      <td>-0.340357</td>\n",
       "      <td>-0.191870</td>\n",
       "      <td>-0.137925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111550</td>\n",
       "      <td>0.049029</td>\n",
       "      <td>0.753282</td>\n",
       "      <td>0.345956</td>\n",
       "      <td>-0.157139</td>\n",
       "      <td>-0.014601</td>\n",
       "      <td>-0.001167</td>\n",
       "      <td>0.547461</td>\n",
       "      <td>-0.253345</td>\n",
       "      <td>0.278555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.219731</td>\n",
       "      <td>0.366471</td>\n",
       "      <td>-0.011441</td>\n",
       "      <td>-0.017548</td>\n",
       "      <td>-0.233026</td>\n",
       "      <td>-0.044800</td>\n",
       "      <td>0.332382</td>\n",
       "      <td>0.340343</td>\n",
       "      <td>0.311558</td>\n",
       "      <td>0.308668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239688</td>\n",
       "      <td>0.129884</td>\n",
       "      <td>-0.061096</td>\n",
       "      <td>0.178418</td>\n",
       "      <td>0.568880</td>\n",
       "      <td>-0.141410</td>\n",
       "      <td>0.327640</td>\n",
       "      <td>-0.053112</td>\n",
       "      <td>0.285459</td>\n",
       "      <td>0.194862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.151244</td>\n",
       "      <td>0.597803</td>\n",
       "      <td>0.068893</td>\n",
       "      <td>0.068871</td>\n",
       "      <td>-0.067664</td>\n",
       "      <td>0.046164</td>\n",
       "      <td>0.121634</td>\n",
       "      <td>0.153979</td>\n",
       "      <td>0.195291</td>\n",
       "      <td>0.444018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498066</td>\n",
       "      <td>0.461643</td>\n",
       "      <td>0.201289</td>\n",
       "      <td>0.622314</td>\n",
       "      <td>0.647801</td>\n",
       "      <td>-0.011690</td>\n",
       "      <td>0.555718</td>\n",
       "      <td>0.274223</td>\n",
       "      <td>0.016616</td>\n",
       "      <td>0.318204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.189126</td>\n",
       "      <td>0.403763</td>\n",
       "      <td>0.011495</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>-0.240423</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>-0.124357</td>\n",
       "      <td>-0.138667</td>\n",
       "      <td>-0.084173</td>\n",
       "      <td>-0.075609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>-0.195691</td>\n",
       "      <td>0.481374</td>\n",
       "      <td>0.452379</td>\n",
       "      <td>0.043169</td>\n",
       "      <td>0.056172</td>\n",
       "      <td>0.114262</td>\n",
       "      <td>0.413340</td>\n",
       "      <td>-0.176149</td>\n",
       "      <td>0.365680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.284386</td>\n",
       "      <td>0.500867</td>\n",
       "      <td>-0.027740</td>\n",
       "      <td>-0.024515</td>\n",
       "      <td>-0.148302</td>\n",
       "      <td>-0.059986</td>\n",
       "      <td>0.030335</td>\n",
       "      <td>0.041796</td>\n",
       "      <td>0.133330</td>\n",
       "      <td>0.309054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146314</td>\n",
       "      <td>0.662688</td>\n",
       "      <td>0.069432</td>\n",
       "      <td>0.291744</td>\n",
       "      <td>0.509387</td>\n",
       "      <td>0.161627</td>\n",
       "      <td>0.449893</td>\n",
       "      <td>0.371164</td>\n",
       "      <td>0.028070</td>\n",
       "      <td>0.163891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.135351</td>\n",
       "      <td>0.455832</td>\n",
       "      <td>-0.004502</td>\n",
       "      <td>-0.003774</td>\n",
       "      <td>-0.136440</td>\n",
       "      <td>0.006008</td>\n",
       "      <td>-0.238645</td>\n",
       "      <td>-0.237474</td>\n",
       "      <td>-0.124887</td>\n",
       "      <td>-0.058543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009489</td>\n",
       "      <td>0.097089</td>\n",
       "      <td>0.622025</td>\n",
       "      <td>0.413727</td>\n",
       "      <td>-0.114439</td>\n",
       "      <td>-0.096245</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.657218</td>\n",
       "      <td>-0.197855</td>\n",
       "      <td>0.333991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.227999</td>\n",
       "      <td>0.468968</td>\n",
       "      <td>0.088340</td>\n",
       "      <td>0.091465</td>\n",
       "      <td>0.111461</td>\n",
       "      <td>0.077207</td>\n",
       "      <td>-0.422463</td>\n",
       "      <td>-0.449526</td>\n",
       "      <td>-0.244600</td>\n",
       "      <td>-0.162225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317757</td>\n",
       "      <td>-0.146600</td>\n",
       "      <td>0.405777</td>\n",
       "      <td>0.231880</td>\n",
       "      <td>-0.050475</td>\n",
       "      <td>0.216234</td>\n",
       "      <td>0.033374</td>\n",
       "      <td>0.240878</td>\n",
       "      <td>-0.237443</td>\n",
       "      <td>0.366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.116910</td>\n",
       "      <td>0.626063</td>\n",
       "      <td>0.128928</td>\n",
       "      <td>0.129491</td>\n",
       "      <td>-0.140045</td>\n",
       "      <td>0.111681</td>\n",
       "      <td>-0.016992</td>\n",
       "      <td>-0.034170</td>\n",
       "      <td>0.007352</td>\n",
       "      <td>0.041880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234248</td>\n",
       "      <td>0.055318</td>\n",
       "      <td>0.668440</td>\n",
       "      <td>0.858452</td>\n",
       "      <td>0.180553</td>\n",
       "      <td>-0.078676</td>\n",
       "      <td>0.311269</td>\n",
       "      <td>0.538216</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.479406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.212090</td>\n",
       "      <td>0.541159</td>\n",
       "      <td>0.024694</td>\n",
       "      <td>0.026252</td>\n",
       "      <td>-0.080522</td>\n",
       "      <td>-0.003104</td>\n",
       "      <td>0.199316</td>\n",
       "      <td>0.227874</td>\n",
       "      <td>0.271884</td>\n",
       "      <td>0.508295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311332</td>\n",
       "      <td>0.638861</td>\n",
       "      <td>0.017328</td>\n",
       "      <td>0.356682</td>\n",
       "      <td>0.884126</td>\n",
       "      <td>0.035570</td>\n",
       "      <td>0.431242</td>\n",
       "      <td>0.219698</td>\n",
       "      <td>0.072938</td>\n",
       "      <td>0.081806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.071974</td>\n",
       "      <td>0.220023</td>\n",
       "      <td>0.044450</td>\n",
       "      <td>0.039037</td>\n",
       "      <td>-0.129110</td>\n",
       "      <td>0.023538</td>\n",
       "      <td>0.425851</td>\n",
       "      <td>0.438670</td>\n",
       "      <td>0.396034</td>\n",
       "      <td>0.253158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233242</td>\n",
       "      <td>0.181264</td>\n",
       "      <td>-0.056840</td>\n",
       "      <td>0.036366</td>\n",
       "      <td>0.489372</td>\n",
       "      <td>-0.127047</td>\n",
       "      <td>0.172974</td>\n",
       "      <td>-0.197200</td>\n",
       "      <td>0.347937</td>\n",
       "      <td>-0.141489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.021045</td>\n",
       "      <td>0.600987</td>\n",
       "      <td>0.006699</td>\n",
       "      <td>0.009451</td>\n",
       "      <td>-0.103876</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>-0.414656</td>\n",
       "      <td>-0.441990</td>\n",
       "      <td>-0.248548</td>\n",
       "      <td>-0.226146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090272</td>\n",
       "      <td>0.013803</td>\n",
       "      <td>0.641860</td>\n",
       "      <td>0.368082</td>\n",
       "      <td>-0.059628</td>\n",
       "      <td>0.130991</td>\n",
       "      <td>-0.088971</td>\n",
       "      <td>0.646396</td>\n",
       "      <td>-0.228660</td>\n",
       "      <td>0.227833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.107039</td>\n",
       "      <td>0.486030</td>\n",
       "      <td>0.017642</td>\n",
       "      <td>0.018478</td>\n",
       "      <td>-0.126733</td>\n",
       "      <td>-0.019199</td>\n",
       "      <td>0.292538</td>\n",
       "      <td>0.299888</td>\n",
       "      <td>0.298524</td>\n",
       "      <td>0.274011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315837</td>\n",
       "      <td>0.422688</td>\n",
       "      <td>0.110374</td>\n",
       "      <td>0.338953</td>\n",
       "      <td>0.515702</td>\n",
       "      <td>-0.117669</td>\n",
       "      <td>0.638995</td>\n",
       "      <td>0.184412</td>\n",
       "      <td>0.352786</td>\n",
       "      <td>0.307817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.181765</td>\n",
       "      <td>0.448741</td>\n",
       "      <td>0.033138</td>\n",
       "      <td>0.029439</td>\n",
       "      <td>-0.170042</td>\n",
       "      <td>0.005930</td>\n",
       "      <td>0.414982</td>\n",
       "      <td>0.432726</td>\n",
       "      <td>0.405081</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291277</td>\n",
       "      <td>0.457378</td>\n",
       "      <td>-0.009782</td>\n",
       "      <td>0.308932</td>\n",
       "      <td>0.711537</td>\n",
       "      <td>-0.133045</td>\n",
       "      <td>0.493525</td>\n",
       "      <td>0.041082</td>\n",
       "      <td>0.323995</td>\n",
       "      <td>0.140628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.092562</td>\n",
       "      <td>0.590402</td>\n",
       "      <td>-0.002744</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>-0.184695</td>\n",
       "      <td>-0.059049</td>\n",
       "      <td>-0.096154</td>\n",
       "      <td>-0.113009</td>\n",
       "      <td>-0.034937</td>\n",
       "      <td>0.082384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361482</td>\n",
       "      <td>0.161185</td>\n",
       "      <td>0.341369</td>\n",
       "      <td>0.523258</td>\n",
       "      <td>0.254828</td>\n",
       "      <td>0.014855</td>\n",
       "      <td>0.552533</td>\n",
       "      <td>0.452903</td>\n",
       "      <td>-0.116168</td>\n",
       "      <td>0.735055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.144722</td>\n",
       "      <td>0.463270</td>\n",
       "      <td>0.052839</td>\n",
       "      <td>0.053075</td>\n",
       "      <td>-0.166696</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.269022</td>\n",
       "      <td>0.294992</td>\n",
       "      <td>0.301978</td>\n",
       "      <td>0.370714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.224744</td>\n",
       "      <td>0.045545</td>\n",
       "      <td>0.323195</td>\n",
       "      <td>0.791011</td>\n",
       "      <td>-0.110251</td>\n",
       "      <td>0.481498</td>\n",
       "      <td>-0.035368</td>\n",
       "      <td>0.152006</td>\n",
       "      <td>0.216773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.276593</td>\n",
       "      <td>0.174728</td>\n",
       "      <td>-0.225801</td>\n",
       "      <td>-0.223514</td>\n",
       "      <td>0.226659</td>\n",
       "      <td>-0.213077</td>\n",
       "      <td>-0.348600</td>\n",
       "      <td>-0.348448</td>\n",
       "      <td>-0.182145</td>\n",
       "      <td>-0.223565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199016</td>\n",
       "      <td>0.083539</td>\n",
       "      <td>0.271239</td>\n",
       "      <td>-0.039035</td>\n",
       "      <td>-0.149637</td>\n",
       "      <td>0.047121</td>\n",
       "      <td>-0.068909</td>\n",
       "      <td>0.009493</td>\n",
       "      <td>-0.182946</td>\n",
       "      <td>-0.117452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.325683</td>\n",
       "      <td>0.446086</td>\n",
       "      <td>0.101054</td>\n",
       "      <td>0.101464</td>\n",
       "      <td>-0.112644</td>\n",
       "      <td>0.122908</td>\n",
       "      <td>-0.140835</td>\n",
       "      <td>-0.124616</td>\n",
       "      <td>-0.036915</td>\n",
       "      <td>0.260657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078181</td>\n",
       "      <td>0.354962</td>\n",
       "      <td>0.262378</td>\n",
       "      <td>0.398101</td>\n",
       "      <td>0.211885</td>\n",
       "      <td>0.111502</td>\n",
       "      <td>0.033564</td>\n",
       "      <td>0.600726</td>\n",
       "      <td>-0.078087</td>\n",
       "      <td>0.202120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.238289</td>\n",
       "      <td>0.408296</td>\n",
       "      <td>0.057824</td>\n",
       "      <td>0.062134</td>\n",
       "      <td>0.153915</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>-0.093445</td>\n",
       "      <td>-0.079379</td>\n",
       "      <td>0.048653</td>\n",
       "      <td>0.101415</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167167</td>\n",
       "      <td>0.237079</td>\n",
       "      <td>0.288348</td>\n",
       "      <td>0.299769</td>\n",
       "      <td>-0.047647</td>\n",
       "      <td>0.497199</td>\n",
       "      <td>-0.059153</td>\n",
       "      <td>-0.129304</td>\n",
       "      <td>0.232506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.141079</td>\n",
       "      <td>0.339493</td>\n",
       "      <td>-0.055608</td>\n",
       "      <td>-0.054072</td>\n",
       "      <td>0.056141</td>\n",
       "      <td>-0.051911</td>\n",
       "      <td>0.099466</td>\n",
       "      <td>0.126370</td>\n",
       "      <td>0.209222</td>\n",
       "      <td>0.279416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050157</td>\n",
       "      <td>0.216226</td>\n",
       "      <td>0.287898</td>\n",
       "      <td>-0.006682</td>\n",
       "      <td>0.363361</td>\n",
       "      <td>0.304340</td>\n",
       "      <td>-0.013475</td>\n",
       "      <td>-0.051631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.014041</td>\n",
       "      <td>0.560328</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>-0.094497</td>\n",
       "      <td>-0.003344</td>\n",
       "      <td>-0.231918</td>\n",
       "      <td>-0.250841</td>\n",
       "      <td>-0.106415</td>\n",
       "      <td>-0.149579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237079</td>\n",
       "      <td>0.050157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.494292</td>\n",
       "      <td>-0.014895</td>\n",
       "      <td>-0.053544</td>\n",
       "      <td>0.143477</td>\n",
       "      <td>0.486194</td>\n",
       "      <td>-0.196604</td>\n",
       "      <td>0.361537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.122167</td>\n",
       "      <td>0.631811</td>\n",
       "      <td>0.139223</td>\n",
       "      <td>0.141646</td>\n",
       "      <td>-0.072074</td>\n",
       "      <td>0.126638</td>\n",
       "      <td>0.029693</td>\n",
       "      <td>0.028196</td>\n",
       "      <td>0.064280</td>\n",
       "      <td>0.207532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288348</td>\n",
       "      <td>0.216226</td>\n",
       "      <td>0.494292</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.353199</td>\n",
       "      <td>-0.059667</td>\n",
       "      <td>0.413678</td>\n",
       "      <td>0.554947</td>\n",
       "      <td>-0.047018</td>\n",
       "      <td>0.461478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.182019</td>\n",
       "      <td>0.516253</td>\n",
       "      <td>0.082991</td>\n",
       "      <td>0.084914</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>0.043272</td>\n",
       "      <td>0.217328</td>\n",
       "      <td>0.237765</td>\n",
       "      <td>0.248623</td>\n",
       "      <td>0.480209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299769</td>\n",
       "      <td>0.287898</td>\n",
       "      <td>-0.014895</td>\n",
       "      <td>0.353199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027467</td>\n",
       "      <td>0.393149</td>\n",
       "      <td>0.105414</td>\n",
       "      <td>0.146407</td>\n",
       "      <td>0.156946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.167571</td>\n",
       "      <td>0.315326</td>\n",
       "      <td>-0.169752</td>\n",
       "      <td>-0.166304</td>\n",
       "      <td>-0.220223</td>\n",
       "      <td>-0.201778</td>\n",
       "      <td>-0.447725</td>\n",
       "      <td>-0.466282</td>\n",
       "      <td>-0.332443</td>\n",
       "      <td>-0.040215</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047647</td>\n",
       "      <td>-0.006682</td>\n",
       "      <td>-0.053544</td>\n",
       "      <td>-0.059667</td>\n",
       "      <td>0.027467</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.079568</td>\n",
       "      <td>0.135137</td>\n",
       "      <td>-0.231507</td>\n",
       "      <td>0.015191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.043977</td>\n",
       "      <td>0.418234</td>\n",
       "      <td>0.033049</td>\n",
       "      <td>0.037505</td>\n",
       "      <td>-0.115186</td>\n",
       "      <td>-0.024720</td>\n",
       "      <td>0.135681</td>\n",
       "      <td>0.143559</td>\n",
       "      <td>0.177048</td>\n",
       "      <td>0.207686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497199</td>\n",
       "      <td>0.363361</td>\n",
       "      <td>0.143477</td>\n",
       "      <td>0.413678</td>\n",
       "      <td>0.393149</td>\n",
       "      <td>-0.079568</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.092365</td>\n",
       "      <td>0.031057</td>\n",
       "      <td>0.446836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.321889</td>\n",
       "      <td>0.631251</td>\n",
       "      <td>-0.017337</td>\n",
       "      <td>-0.017420</td>\n",
       "      <td>-0.307936</td>\n",
       "      <td>-0.024290</td>\n",
       "      <td>-0.194180</td>\n",
       "      <td>-0.203930</td>\n",
       "      <td>-0.086194</td>\n",
       "      <td>-0.042101</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059153</td>\n",
       "      <td>0.304340</td>\n",
       "      <td>0.486194</td>\n",
       "      <td>0.554947</td>\n",
       "      <td>0.105414</td>\n",
       "      <td>0.135137</td>\n",
       "      <td>0.092365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.127132</td>\n",
       "      <td>0.357702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.260855</td>\n",
       "      <td>-0.070284</td>\n",
       "      <td>0.231881</td>\n",
       "      <td>0.230313</td>\n",
       "      <td>0.187330</td>\n",
       "      <td>0.264607</td>\n",
       "      <td>0.654221</td>\n",
       "      <td>0.622073</td>\n",
       "      <td>0.473402</td>\n",
       "      <td>0.030940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129304</td>\n",
       "      <td>-0.013475</td>\n",
       "      <td>-0.196604</td>\n",
       "      <td>-0.047018</td>\n",
       "      <td>0.146407</td>\n",
       "      <td>-0.231507</td>\n",
       "      <td>0.031057</td>\n",
       "      <td>-0.127132</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.150568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.172229</td>\n",
       "      <td>0.494303</td>\n",
       "      <td>0.115093</td>\n",
       "      <td>0.118114</td>\n",
       "      <td>-0.282353</td>\n",
       "      <td>0.048090</td>\n",
       "      <td>-0.095970</td>\n",
       "      <td>-0.126762</td>\n",
       "      <td>-0.078167</td>\n",
       "      <td>0.070622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232506</td>\n",
       "      <td>-0.051631</td>\n",
       "      <td>0.361537</td>\n",
       "      <td>0.461478</td>\n",
       "      <td>0.156946</td>\n",
       "      <td>0.015191</td>\n",
       "      <td>0.446836</td>\n",
       "      <td>0.357702</td>\n",
       "      <td>-0.150568</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7   \\\n",
       "1   1.000000 -0.192000  0.028072  0.033332  0.693835  0.093519  0.265218   \n",
       "2  -0.192000  1.000000 -0.004576 -0.002516 -0.276600 -0.052171 -0.195711   \n",
       "3   0.028072 -0.004576  1.000000  0.999635  0.115671  0.981838  0.148160   \n",
       "4   0.033332 -0.002516  0.999635  1.000000  0.115602  0.978863  0.144013   \n",
       "5   0.693835 -0.276600  0.115671  0.115602  1.000000  0.235160  0.238894   \n",
       "6   0.093519 -0.052171  0.981838  0.978863  0.235160  1.000000  0.196263   \n",
       "7   0.265218 -0.195711  0.148160  0.144013  0.238894  0.196263  1.000000   \n",
       "8   0.230291 -0.206691  0.132954  0.127622  0.257863  0.188561  0.989281   \n",
       "9   0.224367 -0.010387  0.011847  0.010173  0.237655  0.056667  0.722834   \n",
       "10 -0.255230  0.152875  0.436662  0.430959 -0.018931  0.437328  0.160087   \n",
       "11 -0.174986  0.516696 -0.112593 -0.111705 -0.287176 -0.118538 -0.458702   \n",
       "12 -0.231308  0.620085  0.140608  0.143014 -0.149877  0.099519  0.154098   \n",
       "13  0.251006  0.428826 -0.075485 -0.068866  0.141796 -0.095877 -0.523346   \n",
       "14  0.023986  0.469636 -0.035573 -0.034890 -0.017807 -0.018516 -0.327769   \n",
       "15 -0.219731  0.366471 -0.011441 -0.017548 -0.233026 -0.044800  0.332382   \n",
       "16 -0.151244  0.597803  0.068893  0.068871 -0.067664  0.046164  0.121634   \n",
       "17 -0.189126  0.403763  0.011495  0.007939 -0.240423  0.002474 -0.124357   \n",
       "18 -0.284386  0.500867 -0.027740 -0.024515 -0.148302 -0.059986  0.030335   \n",
       "19 -0.135351  0.455832 -0.004502 -0.003774 -0.136440  0.006008 -0.238645   \n",
       "20  0.227999  0.468968  0.088340  0.091465  0.111461  0.077207 -0.422463   \n",
       "21 -0.116910  0.626063  0.128928  0.129491 -0.140045  0.111681 -0.016992   \n",
       "22 -0.212090  0.541159  0.024694  0.026252 -0.080522 -0.003104  0.199316   \n",
       "23 -0.071974  0.220023  0.044450  0.039037 -0.129110  0.023538  0.425851   \n",
       "24  0.021045  0.600987  0.006699  0.009451 -0.103876  0.000513 -0.414656   \n",
       "25 -0.107039  0.486030  0.017642  0.018478 -0.126733 -0.019199  0.292538   \n",
       "26 -0.181765  0.448741  0.033138  0.029439 -0.170042  0.005930  0.414982   \n",
       "27 -0.092562  0.590402 -0.002744  0.003347 -0.184695 -0.059049 -0.096154   \n",
       "28 -0.144722  0.463270  0.052839  0.053075 -0.166696  0.001749  0.269022   \n",
       "29  0.276593  0.174728 -0.225801 -0.223514  0.226659 -0.213077 -0.348600   \n",
       "30 -0.325683  0.446086  0.101054  0.101464 -0.112644  0.122908 -0.140835   \n",
       "31  0.238289  0.408296  0.057824  0.062134  0.153915  0.020962 -0.093445   \n",
       "32 -0.141079  0.339493 -0.055608 -0.054072  0.056141 -0.051911  0.099466   \n",
       "33 -0.014041  0.560328  0.002281  0.002888 -0.094497 -0.003344 -0.231918   \n",
       "34 -0.122167  0.631811  0.139223  0.141646 -0.072074  0.126638  0.029693   \n",
       "35 -0.182019  0.516253  0.082991  0.084914 -0.109221  0.043272  0.217328   \n",
       "36 -0.167571  0.315326 -0.169752 -0.166304 -0.220223 -0.201778 -0.447725   \n",
       "37 -0.043977  0.418234  0.033049  0.037505 -0.115186 -0.024720  0.135681   \n",
       "38 -0.321889  0.631251 -0.017337 -0.017420 -0.307936 -0.024290 -0.194180   \n",
       "39  0.260855 -0.070284  0.231881  0.230313  0.187330  0.264607  0.654221   \n",
       "40 -0.172229  0.494303  0.115093  0.118114 -0.282353  0.048090 -0.095970   \n",
       "\n",
       "          8         9         10  ...        31        32        33        34  \\\n",
       "1   0.230291  0.224367 -0.255230  ...  0.238289 -0.141079 -0.014041 -0.122167   \n",
       "2  -0.206691 -0.010387  0.152875  ...  0.408296  0.339493  0.560328  0.631811   \n",
       "3   0.132954  0.011847  0.436662  ...  0.057824 -0.055608  0.002281  0.139223   \n",
       "4   0.127622  0.010173  0.430959  ...  0.062134 -0.054072  0.002888  0.141646   \n",
       "5   0.257863  0.237655 -0.018931  ...  0.153915  0.056141 -0.094497 -0.072074   \n",
       "6   0.188561  0.056667  0.437328  ...  0.020962 -0.051911 -0.003344  0.126638   \n",
       "7   0.989281  0.722834  0.160087  ... -0.093445  0.099466 -0.231918  0.029693   \n",
       "8   1.000000  0.730466  0.217428  ... -0.079379  0.126370 -0.250841  0.028196   \n",
       "9   0.730466  1.000000  0.065224  ...  0.048653  0.209222 -0.106415  0.064280   \n",
       "10  0.217428  0.065224  1.000000  ...  0.101415  0.279416 -0.149579  0.207532   \n",
       "11 -0.477018 -0.305232 -0.231900  ... -0.162299 -0.012380  0.464235  0.185894   \n",
       "12  0.163338  0.205343  0.388576  ...  0.306133  0.465113  0.199596  0.618593   \n",
       "13 -0.533989 -0.275460 -0.171296  ...  0.348507  0.018992  0.282084  0.087681   \n",
       "14 -0.340357 -0.191870 -0.137925  ...  0.111550  0.049029  0.753282  0.345956   \n",
       "15  0.340343  0.311558  0.308668  ...  0.239688  0.129884 -0.061096  0.178418   \n",
       "16  0.153979  0.195291  0.444018  ...  0.498066  0.461643  0.201289  0.622314   \n",
       "17 -0.138667 -0.084173 -0.075609  ...  0.006732 -0.195691  0.481374  0.452379   \n",
       "18  0.041796  0.133330  0.309054  ...  0.146314  0.662688  0.069432  0.291744   \n",
       "19 -0.237474 -0.124887 -0.058543  ...  0.009489  0.097089  0.622025  0.413727   \n",
       "20 -0.449526 -0.244600 -0.162225  ...  0.317757 -0.146600  0.405777  0.231880   \n",
       "21 -0.034170  0.007352  0.041880  ...  0.234248  0.055318  0.668440  0.858452   \n",
       "22  0.227874  0.271884  0.508295  ...  0.311332  0.638861  0.017328  0.356682   \n",
       "23  0.438670  0.396034  0.253158  ...  0.233242  0.181264 -0.056840  0.036366   \n",
       "24 -0.441990 -0.248548 -0.226146  ...  0.090272  0.013803  0.641860  0.368082   \n",
       "25  0.299888  0.298524  0.274011  ...  0.315837  0.422688  0.110374  0.338953   \n",
       "26  0.432726  0.405081  0.456022  ...  0.291277  0.457378 -0.009782  0.308932   \n",
       "27 -0.113009 -0.034937  0.082384  ...  0.361482  0.161185  0.341369  0.523258   \n",
       "28  0.294992  0.301978  0.370714  ...  0.505000  0.224744  0.045545  0.323195   \n",
       "29 -0.348448 -0.182145 -0.223565  ...  0.199016  0.083539  0.271239 -0.039035   \n",
       "30 -0.124616 -0.036915  0.260657  ... -0.078181  0.354962  0.262378  0.398101   \n",
       "31 -0.079379  0.048653  0.101415  ...  1.000000  0.167167  0.237079  0.288348   \n",
       "32  0.126370  0.209222  0.279416  ...  0.167167  1.000000  0.050157  0.216226   \n",
       "33 -0.250841 -0.106415 -0.149579  ...  0.237079  0.050157  1.000000  0.494292   \n",
       "34  0.028196  0.064280  0.207532  ...  0.288348  0.216226  0.494292  1.000000   \n",
       "35  0.237765  0.248623  0.480209  ...  0.299769  0.287898 -0.014895  0.353199   \n",
       "36 -0.466282 -0.332443 -0.040215  ... -0.047647 -0.006682 -0.053544 -0.059667   \n",
       "37  0.143559  0.177048  0.207686  ...  0.497199  0.363361  0.143477  0.413678   \n",
       "38 -0.203930 -0.086194 -0.042101  ... -0.059153  0.304340  0.486194  0.554947   \n",
       "39  0.622073  0.473402  0.030940  ... -0.129304 -0.013475 -0.196604 -0.047018   \n",
       "40 -0.126762 -0.078167  0.070622  ...  0.232506 -0.051631  0.361537  0.461478   \n",
       "\n",
       "          35        36        37        38        39        40  \n",
       "1  -0.182019 -0.167571 -0.043977 -0.321889  0.260855 -0.172229  \n",
       "2   0.516253  0.315326  0.418234  0.631251 -0.070284  0.494303  \n",
       "3   0.082991 -0.169752  0.033049 -0.017337  0.231881  0.115093  \n",
       "4   0.084914 -0.166304  0.037505 -0.017420  0.230313  0.118114  \n",
       "5  -0.109221 -0.220223 -0.115186 -0.307936  0.187330 -0.282353  \n",
       "6   0.043272 -0.201778 -0.024720 -0.024290  0.264607  0.048090  \n",
       "7   0.217328 -0.447725  0.135681 -0.194180  0.654221 -0.095970  \n",
       "8   0.237765 -0.466282  0.143559 -0.203930  0.622073 -0.126762  \n",
       "9   0.248623 -0.332443  0.177048 -0.086194  0.473402 -0.078167  \n",
       "10  0.480209 -0.040215  0.207686 -0.042101  0.030940  0.070622  \n",
       "11 -0.129755  0.449549 -0.173424  0.578557 -0.245426  0.112362  \n",
       "12  0.692945 -0.039766  0.567402  0.391891  0.116025  0.406802  \n",
       "13 -0.028771  0.396340 -0.023487  0.094099 -0.334810  0.026669  \n",
       "14 -0.157139 -0.014601 -0.001167  0.547461 -0.253345  0.278555  \n",
       "15  0.568880 -0.141410  0.327640 -0.053112  0.285459  0.194862  \n",
       "16  0.647801 -0.011690  0.555718  0.274223  0.016616  0.318204  \n",
       "17  0.043169  0.056172  0.114262  0.413340 -0.176149  0.365680  \n",
       "18  0.509387  0.161627  0.449893  0.371164  0.028070  0.163891  \n",
       "19 -0.114439 -0.096245  0.001532  0.657218 -0.197855  0.333991  \n",
       "20 -0.050475  0.216234  0.033374  0.240878 -0.237443  0.366400  \n",
       "21  0.180553 -0.078676  0.311269  0.538216 -0.002316  0.479406  \n",
       "22  0.884126  0.035570  0.431242  0.219698  0.072938  0.081806  \n",
       "23  0.489372 -0.127047  0.172974 -0.197200  0.347937 -0.141489  \n",
       "24 -0.059628  0.130991 -0.088971  0.646396 -0.228660  0.227833  \n",
       "25  0.515702 -0.117669  0.638995  0.184412  0.352786  0.307817  \n",
       "26  0.711537 -0.133045  0.493525  0.041082  0.323995  0.140628  \n",
       "27  0.254828  0.014855  0.552533  0.452903 -0.116168  0.735055  \n",
       "28  0.791011 -0.110251  0.481498 -0.035368  0.152006  0.216773  \n",
       "29 -0.149637  0.047121 -0.068909  0.009493 -0.182946 -0.117452  \n",
       "30  0.211885  0.111502  0.033564  0.600726 -0.078087  0.202120  \n",
       "31  0.299769 -0.047647  0.497199 -0.059153 -0.129304  0.232506  \n",
       "32  0.287898 -0.006682  0.363361  0.304340 -0.013475 -0.051631  \n",
       "33 -0.014895 -0.053544  0.143477  0.486194 -0.196604  0.361537  \n",
       "34  0.353199 -0.059667  0.413678  0.554947 -0.047018  0.461478  \n",
       "35  1.000000  0.027467  0.393149  0.105414  0.146407  0.156946  \n",
       "36  0.027467  1.000000 -0.079568  0.135137 -0.231507  0.015191  \n",
       "37  0.393149 -0.079568  1.000000  0.092365  0.031057  0.446836  \n",
       "38  0.105414  0.135137  0.092365  1.000000 -0.127132  0.357702  \n",
       "39  0.146407 -0.231507  0.031057 -0.127132  1.000000 -0.150568  \n",
       "40  0.156946  0.015191  0.446836  0.357702 -0.150568  1.000000  \n",
       "\n",
       "[40 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.DataFrame(np.corrcoef(X_train.T))\n",
    "sub_data = data.loc[:,1:]\n",
    "sub_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d474b2",
   "metadata": {},
   "source": [
    "## L1 Regularized Logistic Regression Over Range of Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b36b76",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m alphas:\n\u001b[1;32m      9\u001b[0m     LogR_L1\u001b[38;5;241m.\u001b[39mset_params(C\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39ma))\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mLogR_L1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     pred \u001b[38;5;241m=\u001b[39m LogR_L1\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[1;32m     12\u001b[0m     error \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(pred))\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(pred\u001b[38;5;241m==\u001b[39my_train)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1589\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1588\u001b[0m     prefer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocesses\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1589\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1617\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:864\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    861\u001b[0m         alpha \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m l1_ratio)\n\u001b[1;32m    862\u001b[0m         beta \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C) \u001b[38;5;241m*\u001b[39m l1_ratio\n\u001b[0;32m--> 864\u001b[0m     w0, n_iter_i, warm_start_sag \u001b[38;5;241m=\u001b[39m \u001b[43msag_solver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarm_start_sag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_saga\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolver must be one of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}, got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m solver\n\u001b[1;32m    885\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:327\u001b[0m, in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent sag implementation does not handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe case step_size * alpha_scaled == 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    326\u001b[0m sag \u001b[38;5;241m=\u001b[39m sag64 \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64 \u001b[38;5;28;01melse\u001b[39;00m sag32\n\u001b[0;32m--> 327\u001b[0m num_seen, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43msag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43msum_gradient_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_memory_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseen_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_seen_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_sum_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_saga\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter_ \u001b[38;5;241m==\u001b[39m max_iter:\n\u001b[1;32m    352\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe max_iter was reached which means the coef_ did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    354\u001b[0m         ConvergenceWarning,\n\u001b[1;32m    355\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alphas = np.linspace(0.001,500,100000)\n",
    "#alphas = np.linspace(1,100, 20)\n",
    "LogR_L1 = LogisticRegression(max_iter = 10000, penalty = 'l1', solver = 'saga')\n",
    "coefs = []\n",
    "scores = []\n",
    "errors = []\n",
    "\n",
    "for a in alphas:\n",
    "    LogR_L1.set_params(C=(1/a))\n",
    "    LogR_L1.fit(X_train, y_train)\n",
    "    pred = LogR_L1.predict(X_train)\n",
    "    error = (1/len(pred))*np.sum(pred==y_train)\n",
    "    \n",
    "    scores.append(LogR_L1.score(X_train, y_train))\n",
    "    coefs.append(LogR_L1.coef_[0,:])\n",
    "    errors.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3696857",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Lambda', fontsize = 15)\n",
    "plt.ylabel('Coefficients', fontsize = 15)\n",
    "plt.title('L1 Regularized Coefficients as a Function of Lambda', fontsize = 15)\n",
    "plt.savefig('coefficients_lambda.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe4142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(alphas, scores)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('L1 Regularized Model Accuracy as a Function of Lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(alphas, errors)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Lambda', fontsize = 15)\n",
    "plt.ylabel('Error', fontsize = 15)\n",
    "plt.title('L1 Regularized Logistic Regression Miss-Classification Error', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a137c1",
   "metadata": {},
   "source": [
    "### 5-Fold Cross Validation to Find Optimal Penalization Parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecd7c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model\n",
    "LogR_L1 = LogisticRegression(max_iter = 10000, penalty = 'l1', solver = 'saga')\n",
    "\n",
    "#range of regularization parameters to search\n",
    "alphas = np.linspace(0.01,500,100000)\n",
    "#alphas = np.linspace(1,100,5)\n",
    "\n",
    "#5-fold CV for each regularization parameter\n",
    "kf = KFold(n_splits=5)\n",
    "errors = []\n",
    "for a in alphas:\n",
    "    fold_errors = []\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        #getting CV index\n",
    "        x_tr, x_te = X_train[train_index], X_train[test_index]\n",
    "        y_tr, y_te = np.expand_dims(y_train, -1)[train_index], np.expand_dims(y_train, -1)[test_index]\n",
    "        #y_tr, y_te = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        #fitting model\n",
    "        LogR_L1.set_params(C=(1/a))\n",
    "        LogR_L1.fit(x_tr, y_tr[:,0])\n",
    "        pred = LogR_L1.predict(x_te)\n",
    "        #fold_errors.append((1/len(pred))*np.count_nonzero(pred*y_te))\n",
    "        fold_errors.append((1/len(pred))*np.sum(pred!=y_te))\n",
    "    errors.append(fold_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42f6cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_arr = np.array(errors)\n",
    "avg_errors = np.mean(errors_arr, 1)\n",
    "errors = pd.DataFrame(errors_arr, columns = ['Fold 1 Error', 'Fold 2 Error', 'Fold 3 Error', 'Fold 4 Error', 'Fold 5 Error'])\n",
    "avg_errors = pd.DataFrame(avg_errors, columns = ['Avg Error'])\n",
    "lambdas = pd.DataFrame(alphas, columns = ['Lambda'])\n",
    "cv_results = pd.concat([lambdas, errors, avg_errors], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b4b2201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.919940899408994"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_idx = cv_results.index[cv_results['Avg Error'] > 0].tolist()\n",
    "cv_results_non_zero = cv_results.loc[non_zero_idx]\n",
    "cv_results_non_zero = cv_results_non_zero.drop_duplicates(subset = ['Fold 1 Error', 'Fold 2 Error', 'Fold 3 Error', 'Fold 4 Error', 'Fold 5 Error', 'Avg Error'])\n",
    "lambda_cv = cv_results_non_zero.sort_values('Avg Error').iloc[0,0]\n",
    "lambda_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9270340",
   "metadata": {},
   "source": [
    "## L1, L2, Elastic Net Logistic Regression\n",
    "-Looking at Logistic Regression with same alpha to see consistancy with zero and non-zero coefficients \n",
    "\n",
    "-Finding confidence intervals for each coefficient value\n",
    "\n",
    "Notes: \n",
    "-sklearn Logtistic Regression - smaller C constrain model, larger C gives model more freedom\n",
    "-slide 27 Var Sel 1 - pick lambda to be much larger than n \n",
    "-since C = 1/lambda we want C to be much smaller than 1/24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a38f51f",
   "metadata": {},
   "source": [
    "## Running Logistic Regression with No Penalty, L1, L2, and Elastic Net Penalization\n",
    "\n",
    "- found optimal alpha with CV\n",
    "- multiplied by (lambda_cv*sqrt(log(p)))\n",
    "- redoing Logistic Regression with new lambda boostraping to get Confidence Intervals and p-values of coefficients \n",
    "- this will give me p0\n",
    "- using the optimal number of features, p0, to redo logistic regression with no penalization\n",
    "- these coefficients will give me relevant importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff05c252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.331023047668648\n"
     ]
    }
   ],
   "source": [
    "#lambda_p0 = lambda_cv\n",
    "lambda_p0 = lambda_cv*np.sqrt(np.log(X.shape[1]))\n",
    "print(lambda_p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72a14ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure bootstrap\n",
    "n_iterations = 10000\n",
    "n_size = int(len(X_train) * 0.50)\n",
    "\n",
    "# run bootstrap\n",
    "stats = list()\n",
    "for i in range(n_iterations):\n",
    "    # prepare train and test sets\n",
    "    x_tr, y_tr = resample(X_train, y_train, n_samples=n_size)\n",
    "\n",
    "    # fit model\n",
    "    model = LogisticRegression(penalty = 'l1', max_iter = 10000, solver = 'saga', C = 1/lambda_p0)\n",
    "    model.fit(x_tr, y_tr)\n",
    "    stats.append(model.coef_[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00b0cb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:2162: RuntimeWarning: invalid value encountered in multiply\n",
      "  lower_bound = _a * scale + loc\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/genEnv/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:2163: RuntimeWarning: invalid value encountered in multiply\n",
      "  upper_bound = _b * scale + loc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Confidence Interval</th>\n",
       "      <th>T-Statistic</th>\n",
       "      <th>P-Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.000070</td>\n",
       "      <td>(-0.00014375877565189605, 3.185494910153445e-06)</td>\n",
       "      <td>-1.573684</td>\n",
       "      <td>0.115576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000068</td>\n",
       "      <td>(-9.035647412030138e-06, 0.00014536954686233185)</td>\n",
       "      <td>1.452478</td>\n",
       "      <td>0.146385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000028</td>\n",
       "      <td>(-1.8184166107518606e-05, 7.456864987479644e-05)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.317323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>(-5.822555145007203e-06, 2.387682082409134e-05)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.317323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Mean                               Confidence Interval  T-Statistic  \\\n",
       "19 -0.000070  (-0.00014375877565189605, 3.185494910153445e-06)    -1.573684   \n",
       "2   0.000068  (-9.035647412030138e-06, 0.00014536954686233185)     1.452478   \n",
       "9   0.000028  (-1.8184166107518606e-05, 7.456864987479644e-05)     1.000000   \n",
       "28  0.000009   (-5.822555145007203e-06, 2.387682082409134e-05)     1.000000   \n",
       "0   0.000000                                        (nan, nan)          NaN   \n",
       "1   0.000000                                        (nan, nan)          NaN   \n",
       "3   0.000000                                        (nan, nan)          NaN   \n",
       "4   0.000000                                        (nan, nan)          NaN   \n",
       "5   0.000000                                        (nan, nan)          NaN   \n",
       "6   0.000000                                        (nan, nan)          NaN   \n",
       "7   0.000000                                        (nan, nan)          NaN   \n",
       "8   0.000000                                        (nan, nan)          NaN   \n",
       "10  0.000000                                        (nan, nan)          NaN   \n",
       "11  0.000000                                        (nan, nan)          NaN   \n",
       "12  0.000000                                        (nan, nan)          NaN   \n",
       "13  0.000000                                        (nan, nan)          NaN   \n",
       "14  0.000000                                        (nan, nan)          NaN   \n",
       "15  0.000000                                        (nan, nan)          NaN   \n",
       "16  0.000000                                        (nan, nan)          NaN   \n",
       "17  0.000000                                        (nan, nan)          NaN   \n",
       "18  0.000000                                        (nan, nan)          NaN   \n",
       "20  0.000000                                        (nan, nan)          NaN   \n",
       "21  0.000000                                        (nan, nan)          NaN   \n",
       "22  0.000000                                        (nan, nan)          NaN   \n",
       "23  0.000000                                        (nan, nan)          NaN   \n",
       "24  0.000000                                        (nan, nan)          NaN   \n",
       "25  0.000000                                        (nan, nan)          NaN   \n",
       "26  0.000000                                        (nan, nan)          NaN   \n",
       "27  0.000000                                        (nan, nan)          NaN   \n",
       "29  0.000000                                        (nan, nan)          NaN   \n",
       "\n",
       "     P-Value  \n",
       "19  0.115576  \n",
       "2   0.146385  \n",
       "9   0.317323  \n",
       "28  0.317323  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "5        NaN  \n",
       "6        NaN  \n",
       "7        NaN  \n",
       "8        NaN  \n",
       "10       NaN  \n",
       "11       NaN  \n",
       "12       NaN  \n",
       "13       NaN  \n",
       "14       NaN  \n",
       "15       NaN  \n",
       "16       NaN  \n",
       "17       NaN  \n",
       "18       NaN  \n",
       "20       NaN  \n",
       "21       NaN  \n",
       "22       NaN  \n",
       "23       NaN  \n",
       "24       NaN  \n",
       "25       NaN  \n",
       "26       NaN  \n",
       "27       NaN  \n",
       "29       NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confidence intervals\n",
    "coef_stats = list()\n",
    "stats = np.array(stats)\n",
    "for i in range(stats.shape[1]):\n",
    "    coef_stats.append([np.mean(stats[:,i]), st.t.interval(alpha=0.90, df=len(stats)-1, loc=np.mean(stats[:,i]), scale=st.sem(stats[:,i])), st.ttest_ind(stats[:,i], np.zeros(stats[:,i].shape)).statistic, st.ttest_ind(stats[:,i], np.zeros(stats[:,i].shape)).pvalue])\n",
    "\n",
    "results = pd.DataFrame(coef_stats, columns = ['Mean', 'Confidence Interval', 'T-Statistic', 'P-Value']).sort_values('P-Value')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af65afc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getting subset of features \n",
    "idx = results[results['P-Value'] >= 0].index.tolist()\n",
    "p_0_L1 = pd.DataFrame(X_train).loc[:,idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5623e7d1",
   "metadata": {},
   "source": [
    "## Leave-One-Out Feature Importance \n",
    "- iterate over features\n",
    "    - train l1 regularized logistic regression classifier with p-1 features \n",
    "    - find one with the most miss-classification error - this gives features with more importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b91f6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting training data in to a training and holdout test set to calculate risk\n",
    "X_train_loo, X_test_loo, y_train_loo, y_test_loo = train_test_split(X_train, y_train, test_size=0.33, random_state=0)\n",
    "risks = list()\n",
    "errors = list()\n",
    "for i in range(X_train.shape[1]):\n",
    "    #selecting p-1 features\n",
    "    selector = [x for x in range(X_train_loo.shape[1]) if x != i]\n",
    "    x_tr_feat = X_train_loo[:, selector]\n",
    "    x_te_feat = X_test_loo[:, selector]\n",
    "\n",
    "    #----------Bootstrapped----------\n",
    "    risks_boot = list()\n",
    "    errors_boot = list()\n",
    "    for i in range(10000):\n",
    "        # prepare train and test sets\n",
    "        x_tr_boot_all_feat, x_tr_boot_sel_feat, y_tr_boot = resample(X_train_loo, x_tr_feat, y_train_loo, n_samples=n_size)        \n",
    "        \n",
    "        #----------Feature Selected----------\n",
    "        model = LogisticRegression(max_iter = 10000, solver = 'saga')\n",
    "        #fitting model\n",
    "        model.fit(x_tr_boot_sel_feat, y_tr_boot)\n",
    "        pred_f = model.predict(x_te_feat)\n",
    "        error = (1/len(pred_f))*np.sum(y_test_loo != pred_f)\n",
    "        \n",
    "        #----------All Features----------\n",
    "        model = LogisticRegression(max_iter = 10000, solver = 'saga')\n",
    "        #fitting model\n",
    "        model.fit(x_tr_boot_all_feat, y_tr_boot)\n",
    "        pred = model.predict(X_test_loo)\n",
    "\n",
    "        risk = 0 \n",
    "        for k in range(len(pred)):\n",
    "            risk_f = ((y_test_loo.to_numpy()[k] - pred_f[k])** 2)\n",
    "            risk_m = ((y_test_loo.to_numpy()[k] - pred[k])** 2)\n",
    "            risk += (risk_f - risk_m)\n",
    "        risk = (1/len(pred))*risk\n",
    "        risks_boot.append(risk)\n",
    "        errors_boot.append(error)\n",
    "    risks.append(risks_boot)\n",
    "    errors.append(errors_boot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77826029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 10000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg Risk</th>\n",
       "      <th>Avg Error</th>\n",
       "      <th>Confidence Interval</th>\n",
       "      <th>T-Statistic</th>\n",
       "      <th>P-Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.031580</td>\n",
       "      <td>0.125215</td>\n",
       "      <td>(0.031318699043928824, 0.03184130095607117)</td>\n",
       "      <td>205.351072</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012376</td>\n",
       "      <td>0.106129</td>\n",
       "      <td>(0.012197691673018962, 0.01255341943809215)</td>\n",
       "      <td>118.223220</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.007966</td>\n",
       "      <td>0.101673</td>\n",
       "      <td>(0.007800080682589655, 0.0081317711692622)</td>\n",
       "      <td>81.612953</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007387</td>\n",
       "      <td>0.101255</td>\n",
       "      <td>(0.007216610695592636, 0.007556722637740697)</td>\n",
       "      <td>73.804436</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.005925</td>\n",
       "      <td>0.099964</td>\n",
       "      <td>(0.005782245964977172, 0.0060681244053931984)</td>\n",
       "      <td>70.433029</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.099247</td>\n",
       "      <td>(0.005325454975929903, 0.005553063542588612)</td>\n",
       "      <td>81.209530</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.097831</td>\n",
       "      <td>(0.0037647602163389353, 0.003992276820698102)</td>\n",
       "      <td>57.930679</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.097373</td>\n",
       "      <td>(0.0030709386558808926, 0.0032905428256005884)</td>\n",
       "      <td>49.220218</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.095688</td>\n",
       "      <td>(0.0016981805356970454, 0.0018440416865251764)</td>\n",
       "      <td>41.263115</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.095151</td>\n",
       "      <td>(0.0012983218291393109, 0.0015890855782680967)</td>\n",
       "      <td>16.873052</td>\n",
       "      <td>1.951774e-63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.095114</td>\n",
       "      <td>(0.0011113857515881256, 0.001253058692856319)</td>\n",
       "      <td>28.357507</td>\n",
       "      <td>1.812119e-173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.094845</td>\n",
       "      <td>(0.0010724534898924711, 0.0012312502138112325)</td>\n",
       "      <td>24.649660</td>\n",
       "      <td>3.472910e-132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.094902</td>\n",
       "      <td>(0.0010502397213097943, 0.0012238343527642796)</td>\n",
       "      <td>22.258411</td>\n",
       "      <td>1.937167e-108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.094556</td>\n",
       "      <td>(0.0007650002901226459, 0.0009312960061736502)</td>\n",
       "      <td>17.331913</td>\n",
       "      <td>8.317029e-67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.094147</td>\n",
       "      <td>(0.0005061737223833317, 0.0006790114628018534)</td>\n",
       "      <td>11.651276</td>\n",
       "      <td>2.852788e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.094161</td>\n",
       "      <td>(0.00037608669273873495, 0.0005335429368908947)</td>\n",
       "      <td>9.815910</td>\n",
       "      <td>1.082193e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.094461</td>\n",
       "      <td>(0.00011743546868291416, 0.00022330527205782657)</td>\n",
       "      <td>5.468621</td>\n",
       "      <td>4.589837e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.093804</td>\n",
       "      <td>(1.9207569098930085e-05, 0.00019116280127144024)</td>\n",
       "      <td>2.078715</td>\n",
       "      <td>3.765624e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.000164</td>\n",
       "      <td>0.093779</td>\n",
       "      <td>(-0.00022567498638626372, -0.00010173242102114...</td>\n",
       "      <td>-4.488424</td>\n",
       "      <td>7.215245e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.000237</td>\n",
       "      <td>0.093462</td>\n",
       "      <td>(-0.0003028232771714726, -0.00017125079690260157)</td>\n",
       "      <td>-6.122193</td>\n",
       "      <td>9.401654e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0.093132</td>\n",
       "      <td>(-0.00030871879480024974, -0.00018757750149604...</td>\n",
       "      <td>-6.961049</td>\n",
       "      <td>3.482106e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.000617</td>\n",
       "      <td>0.093276</td>\n",
       "      <td>(-0.0006874285286479402, -0.000546645545426134)</td>\n",
       "      <td>-14.894191</td>\n",
       "      <td>6.657691e-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.000664</td>\n",
       "      <td>0.093215</td>\n",
       "      <td>(-0.0007495497905286104, -0.0005778576168787972)</td>\n",
       "      <td>-13.136497</td>\n",
       "      <td>2.958183e-39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.000879</td>\n",
       "      <td>0.093319</td>\n",
       "      <td>(-0.0009414090725865618, -0.0008171094459319569)</td>\n",
       "      <td>-24.038257</td>\n",
       "      <td>6.748476e-126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.000916</td>\n",
       "      <td>0.092914</td>\n",
       "      <td>(-0.0009709262708659079, -0.0008601848402452031)</td>\n",
       "      <td>-28.095089</td>\n",
       "      <td>2.268114e-170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.000941</td>\n",
       "      <td>0.092953</td>\n",
       "      <td>(-0.0010573032699740569, -0.000825659692988906)</td>\n",
       "      <td>-13.811707</td>\n",
       "      <td>3.420678e-43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.001495</td>\n",
       "      <td>0.092158</td>\n",
       "      <td>(-0.0015705528650053653, -0.001419076764624264)</td>\n",
       "      <td>-33.535063</td>\n",
       "      <td>6.360581e-240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001639</td>\n",
       "      <td>0.092423</td>\n",
       "      <td>(-0.0017455801951883837, -0.0015329383233301346)</td>\n",
       "      <td>-26.197190</td>\n",
       "      <td>9.202694e-149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.002387</td>\n",
       "      <td>0.091545</td>\n",
       "      <td>(-0.0024875640298945767, -0.002285769303438756)</td>\n",
       "      <td>-40.191832</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.006273</td>\n",
       "      <td>0.087408</td>\n",
       "      <td>(-0.006480669726068286, -0.006065996940598382)</td>\n",
       "      <td>-51.410127</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Avg Risk  Avg Error                                Confidence Interval  \\\n",
       "28  0.031580   0.125215        (0.031318699043928824, 0.03184130095607117)   \n",
       "2   0.012376   0.106129        (0.012197691673018962, 0.01255341943809215)   \n",
       "19  0.007966   0.101673         (0.007800080682589655, 0.0081317711692622)   \n",
       "9   0.007387   0.101255       (0.007216610695592636, 0.007556722637740697)   \n",
       "27  0.005925   0.099964      (0.005782245964977172, 0.0060681244053931984)   \n",
       "4   0.005439   0.099247       (0.005325454975929903, 0.005553063542588612)   \n",
       "25  0.003879   0.097831      (0.0037647602163389353, 0.003992276820698102)   \n",
       "3   0.003181   0.097373     (0.0030709386558808926, 0.0032905428256005884)   \n",
       "13  0.001771   0.095688     (0.0016981805356970454, 0.0018440416865251764)   \n",
       "7   0.001444   0.095151     (0.0012983218291393109, 0.0015890855782680967)   \n",
       "1   0.001182   0.095114      (0.0011113857515881256, 0.001253058692856319)   \n",
       "15  0.001152   0.094845     (0.0010724534898924711, 0.0012312502138112325)   \n",
       "29  0.001137   0.094902     (0.0010502397213097943, 0.0012238343527642796)   \n",
       "18  0.000848   0.094556     (0.0007650002901226459, 0.0009312960061736502)   \n",
       "17  0.000593   0.094147     (0.0005061737223833317, 0.0006790114628018534)   \n",
       "12  0.000455   0.094161    (0.00037608669273873495, 0.0005335429368908947)   \n",
       "23  0.000170   0.094461   (0.00011743546868291416, 0.00022330527205782657)   \n",
       "26  0.000105   0.093804   (1.9207569098930085e-05, 0.00019116280127144024)   \n",
       "5  -0.000164   0.093779  (-0.00022567498638626372, -0.00010173242102114...   \n",
       "10 -0.000237   0.093462  (-0.0003028232771714726, -0.00017125079690260157)   \n",
       "16 -0.000248   0.093132  (-0.00030871879480024974, -0.00018757750149604...   \n",
       "22 -0.000617   0.093276    (-0.0006874285286479402, -0.000546645545426134)   \n",
       "21 -0.000664   0.093215   (-0.0007495497905286104, -0.0005778576168787972)   \n",
       "11 -0.000879   0.093319   (-0.0009414090725865618, -0.0008171094459319569)   \n",
       "24 -0.000916   0.092914   (-0.0009709262708659079, -0.0008601848402452031)   \n",
       "6  -0.000941   0.092953    (-0.0010573032699740569, -0.000825659692988906)   \n",
       "8  -0.001495   0.092158    (-0.0015705528650053653, -0.001419076764624264)   \n",
       "0  -0.001639   0.092423   (-0.0017455801951883837, -0.0015329383233301346)   \n",
       "14 -0.002387   0.091545    (-0.0024875640298945767, -0.002285769303438756)   \n",
       "20 -0.006273   0.087408     (-0.006480669726068286, -0.006065996940598382)   \n",
       "\n",
       "    T-Statistic        P-Value  \n",
       "28   205.351072   0.000000e+00  \n",
       "2    118.223220   0.000000e+00  \n",
       "19    81.612953   0.000000e+00  \n",
       "9     73.804436   0.000000e+00  \n",
       "27    70.433029   0.000000e+00  \n",
       "4     81.209530   0.000000e+00  \n",
       "25    57.930679   0.000000e+00  \n",
       "3     49.220218   0.000000e+00  \n",
       "13    41.263115   0.000000e+00  \n",
       "7     16.873052   1.951774e-63  \n",
       "1     28.357507  1.812119e-173  \n",
       "15    24.649660  3.472910e-132  \n",
       "29    22.258411  1.937167e-108  \n",
       "18    17.331913   8.317029e-67  \n",
       "17    11.651276   2.852788e-31  \n",
       "12     9.815910   1.082193e-22  \n",
       "23     5.468621   4.589837e-08  \n",
       "26     2.078715   3.765624e-02  \n",
       "5     -4.488424   7.215245e-06  \n",
       "10    -6.122193   9.401654e-10  \n",
       "16    -6.961049   3.482106e-12  \n",
       "22   -14.894191   6.657691e-50  \n",
       "21   -13.136497   2.958183e-39  \n",
       "11   -24.038257  6.748476e-126  \n",
       "24   -28.095089  2.268114e-170  \n",
       "6    -13.811707   3.420678e-43  \n",
       "8    -33.535063  6.360581e-240  \n",
       "0    -26.197190  9.202694e-149  \n",
       "14   -40.191832   0.000000e+00  \n",
       "20   -51.410127   0.000000e+00  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risks_arr = np.array(risks)\n",
    "avg_risks = np.mean(risks_arr, 1)\n",
    "stats = risks\n",
    "\n",
    "errors_arr = np.array(errors)\n",
    "print(errors_arr.shape)\n",
    "avg_errors = np.mean(errors_arr, 1)\n",
    "\n",
    "\n",
    "# confidence intervals\n",
    "coef_stats = list()\n",
    "for i in range(risks_arr.shape[0]):\n",
    "    coef_stats.append([np.mean(risks_arr[i,:]), np.mean(errors_arr[i,:]), st.t.interval(alpha=0.90, df=len(risks_arr)-1, loc=np.mean(risks_arr[i,:]), scale=st.sem(risks_arr[i,:])), st.ttest_ind(risks_arr[i,:], np.zeros(risks_arr[i,:].shape)).statistic, st.ttest_ind(risks_arr[i,:], np.zeros(risks_arr[i,:].shape)).pvalue])\n",
    "\n",
    "results = pd.DataFrame(coef_stats, columns = ['Avg Risk', 'Avg Error', 'Confidence Interval', 'T-Statistic', 'P-Value']).sort_values('Avg Risk', ascending = False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daf60475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['28', '2', '19', '9', '27', '4', '25', '3', '13', '7', '1', '15', '29', '18', '17', '12', '23', '26', '5', '10', '16', '22', '21', '11', '24', '6', '8', '0', '14', '20']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = results.index.to_numpy()\n",
    "plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})\n",
    "idx_str = [str(x) for x in idx]\n",
    "avg_err = results.iloc[:,1].to_numpy()\n",
    "print(idx_str)\n",
    "ax = plt.gca()\n",
    "ax.plot(avg_err)\n",
    "plt.axis('tight')\n",
    "ax.set_xticks(np.arange(len(idx)))\n",
    "ax.set_xticklabels(idx_str)\n",
    "plt.axvline(9, color = 'r')\n",
    "plt.xlabel('Tissue Type', fontsize = 15)\n",
    "plt.ylabel('Miss Classification Error', fontsize = 15)\n",
    "plt.title('LOCO Avg Bootstrapped Miss Classification Error', fontsize = 15)\n",
    "plt.savefig('/Users/hughtillmanjamesjr./Desktop/Wash U Engineering/2022-2023/Fall 2022/Topics in Stat - ML/Final/loco_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f357a444",
   "metadata": {},
   "source": [
    "## Inference on Test Data Set with Sparse Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751009fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Model \n",
    "LogR = LogisticRegression(max_iter = 10000, solver = 'saga')\n",
    "LogR.fit(X_train, y_train)\n",
    "pred = LogR.predict(X_test)\n",
    "print('--------------------------')\n",
    "print('LogR with All Features')\n",
    "#print('Coefficients', LogR.coef_[0,:])\n",
    "print('Test Accuracy: ', (1/len(pred))*np.sum(y_test == pred))\n",
    "print('Test Miss Class Error: ', (1/len(pred))*np.sum(y_test != pred))\n",
    "\n",
    "#P0 from Logistic Regression Methods\n",
    "LogR_L1 = LogisticRegression(max_iter = 10000, solver = 'saga')\n",
    "LogR_L1.fit(X_train[:,[28, 19, 2, 9]], y_train)\n",
    "pred_L1 = LogR_L1.predict(X_test[:,[28, 19, 2, 9]])\n",
    "print('--------------------------')\n",
    "print('LogR with L1 Penalized Features')\n",
    "#print('Coefficients', LogR_L1.coef_[0,:])\n",
    "print('Test Accuracy: ', (1/len(pred_L1))*np.sum(y_test == pred_L1))\n",
    "print('Test Miss Class Error: ', (1/len(pred_L1))*np.sum(y_test != pred_L1))\n",
    "\n",
    "\n",
    "#P0 from Leave-One-Out Methods\n",
    "LogR_LOCO = LogisticRegression(max_iter = 10000, solver = 'saga')\n",
    "LogR_LOCO.fit(X_train[:,[28, 2, 19, 9, 27, 4, 25, 3, 13, 15]], y_train)\n",
    "pred_LOCO = LogR_LOCO.predict(X_test[:,[28, 2, 19, 9, 27, 4, 25, 3, 13, 15]])\n",
    "print('--------------------------')\n",
    "print('LogR with LOCO Features')\n",
    "#print('Coefficients', LogR_L1.coef_[0,:])\n",
    "print('Test Accuracy: ', (1/len(pred_LOCO))*np.sum(y_test == pred_LOCO))\n",
    "print('Test Miss Class Error: ', (1/len(pred_LOCO))*np.sum(y_test != pred_LOCO))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e397e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402028bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
